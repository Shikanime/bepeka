{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python389jvsc74a57bd051436f76c627f8be8713035cf2c451cb68cf413a7892f6e5abd4ad31fbc489ff",
   "display_name": "Python 3.8.9 64-bit ('3.8.9')",
   "language": "python"
  },
  "metadata": {
   "interpreter": {
    "hash": "df63770a6e32c921f58c3e8350f4a1433cfe774c3637c307e8a6bdb12178c3a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from hentai import Utils, Hentai, Option\n",
    "from pathlib import Path"
   ]
  },
  {
   "source": [
    "## Data Collection\n",
    "\n",
    "We will be using the doujin dataset obtained from nhentai."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLE = 0 # Number of sample to refetch from hentai."
   ]
  },
  {
   "source": [
    "## Download dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df = pd.DataFrame([Utils.get_random_hentai() for _ in range(0, N_SAMPLE)])\n",
    "samples_df = samples_df.apply(lambda x: x.dictionary(Option.all()))\n",
    "data_path = Path(\"data\")\n",
    "metadata_path = data_path / \"metadata.csv\"\n",
    "if not metadata_path.is_file():\n",
    "   samples_df.to_csv(metadata_path, index=False, header=\"column_names\")\n",
    "else:\n",
    "   samples_df.to_csv(metadata_path, index=False, mode=\"a\", header=False)\n",
    "print(\"Number of resampled samples: \", len(samples_df))\n"
   ]
  },
  {
   "source": [
    "## Read dataset file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converters = {\n",
    "    column_name: ast.literal_eval\n",
    "    for column_name in [\"tag\", \"group\", \"parody\", \"character\", \"artist\", \"category\", \"image_urls\"]\n",
    "}\n",
    "hentais_df = pd.read_csv(metadata_path, converters=converters)\n",
    "hentais_df"
   ]
  },
  {
   "source": [
    "## Download images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, hentai in hentais_df.iterrows():\n",
    "    hentai_path = data_path / str(hentai.id)\n",
    "    if not hentai_path.is_dir():\n",
    "        hentai = Hentai(hentai.id)\n",
    "        hentai.download(hentai_path, progressbar=True)"
   ]
  },
  {
   "source": [
    "## Data preparation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_freq = hentais_df[\"tag\"].explode().value_counts().sort_values(ascending=False).head(50)\n",
    "\n",
    "style.use(\"fivethirtyeight\")\n",
    "plt.figure(figsize=(12, 20))\n",
    "sns.barplot(y=label_freq.index.values, x=label_freq, order=label_freq.index)\n",
    "plt.title(\"Label frequency\", fontsize=14)\n",
    "plt.xlabel(\"\")\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Data sparsity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nobjs = 2 # Maximum number of images to display\n",
    "ncols = 2 # Number of columns in display\n",
    "nrows = nobjs // ncols # Number of rows in display\n",
    "plt.figure(figsize=(14, 4 * nrows))\n",
    "hentais_df[\"num_favorites\"].plot.hist(ax=plt.subplot(nrows, ncols, 1), bins=100, title=\"Favorites\")\n",
    "hentais_df[\"num_pages\"].plot.hist(ax=plt.subplot(nrows, ncols, 2), bins=100, title=\"Pages\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Train / val split\n",
    "\n",
    "We need to complete the full path to locate training and test images from the current working directory."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hentais_df = hentais_df.explode(\"image_urls\").reset_index()\n",
    "filenames_df = hentais_df.apply(lambda x: str(Path(str(x[\"id\"])) / Path(x[\"image_urls\"]).name), axis=1).rename(\"filename\")\n",
    "labels_df = hentais_df[\"tag\"].rename(\"labels\")\n",
    "hentais_df = pd.concat([filenames_df, labels_df], axis=1)\n",
    "hentais_df"
   ]
  },
  {
   "source": [
    "Splitting the modeling data into training and validation is common in machine learning practice.\n",
    "We will be allocating 80% of the images for training and 20% for validation.\n",
    "Usually, we keep a final test set to communicate performance results but we will not really need it in this notebook."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(hentais_df, test_size=0.2, random_state=44)\n",
    "print(\"Number of hentais for training: \", len(train_df))\n",
    "print(\"Number of hentais for validation: \", len(val_df))"
   ]
  },
  {
   "source": [
    "## Image examples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nobjs = 8 # Maximum number of images to display\n",
    "ncols = 4 # Number of columns in display\n",
    "nrows = nobjs // ncols # Number of rows in display\n",
    "samples = hentais_df[\"filename\"].explode().apply(lambda x: str(data_path / x)).sample(nrows * ncols)\n",
    "plt.figure(figsize=(14, 4 * nrows))\n",
    "for i, img in enumerate(samples):\n",
    "    ax = plt.subplot(nrows, ncols, i+1)\n",
    "    ax.imshow(plt.imread(img, format=\"jpeg\"))"
   ]
  },
  {
   "source": [
    "## Label encoding\n",
    "\n",
    "The original targets are lists of strings that can be easily understood by humans.\n",
    "But, if we want to build and train a neural network we need to create binary labels (multi-hot encoding).\n",
    "This is critical for multi-label classification.\n",
    "\n",
    "In order to binarize our labels, we will be using scikit-learn's MultiLabelBinarizer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the multi-label binarizer on the training set\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(hentais_df[\"labels\"])\n",
    "\n",
    "# Loop over all labels and show them\n",
    "nlabels = len(mlb.classes_)\n",
    "\n",
    "pd.DataFrame({\"labels\": mlb.classes_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the targets of the training and test sets\n",
    "y_train_bin = mlb.transform(train_df[\"labels\"])\n",
    "y_val_bin = mlb.transform(val_df[\"labels\"])"
   ]
  },
  {
   "source": [
    "Let's check if everything worked correctly (We should obtain binary targets instead of list of strings)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print example of hentai tags and their binary targets\n",
    "pd.DataFrame(zip(train_df[\"filename\"], y_train_bin), columns=[\"filename\", \"labels\"])"
   ]
  },
  {
   "source": [
    "## Tensorflow DataSet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = keras.preprocessing.image.ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256 # Big enough to measure an F1-score\n",
    "IMG_SIZE = 224 # Specify height and width of image to match the input format of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_gen.flow_from_dataframe(\n",
    "    dataframe=hentais_df,\n",
    "    directory=\"data\",\n",
    "    x_col=\"filename\",\n",
    "    y_col=\"labels\",\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    shuffle=True,\n",
    "    seed=44,\n",
    "    subset='training'\n",
    ")\n",
    "val_ds = train_gen.flow_from_dataframe(\n",
    "    dataframe=hentais_df,\n",
    "    directory=\"data\",\n",
    "    x_col=\"filename\",\n",
    "    y_col=\"labels\",\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    seed=44,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "source": [
    "## Transfert learning feature extractor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "cnn_model = keras.applications.InceptionV3(include_top=False, weights=\"imagenet\", pooling=\"max\")\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    layers.InputLayer(input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "    cnn_model,\n",
    "    layers.Dense(1024, activation=\"relu\"),\n",
    "    layers.Dropout(.5),\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dropout(.5),\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dropout(.5),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(nlabels, activation=\"softmax\")\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "## Train the model\n",
    "Specify the learning rate and the number of training epochs (number of loops over the whole dataset)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-5 # Keep it small when transfer learning\n",
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=LR), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}