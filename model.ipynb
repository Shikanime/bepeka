{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd07709ee700c0b35b705593aafecb5816646a1f77b192913c2314f04e0eb97f79e",
   "display_name": "Python 3.7.10 64-bit ('bepeka': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "df63770a6e32c921f58c3e8350f4a1433cfe774c3637c307e8a6bdb12178c3a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import seaborn as sns\n",
    "from PIL import Image, ImageFile\n",
    "from datetime import datetime\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from hentai import Utils, Hentai, Option\n",
    "from pathlib import Path"
   ]
  },
  {
   "source": [
    "## Package flags"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n"
   ]
  },
  {
   "source": [
    "## Data Collection\n",
    "\n",
    "We will be using the doujin dataset obtained from nhentai."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLE = 0 # Number of sample to refetch from hentai."
   ]
  },
  {
   "source": [
    "## Download dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df = pd.DataFrame([Utils.get_random_hentai() for _ in range(0, N_SAMPLE)])\n",
    "samples_df = samples_df.apply(lambda x: x.dictionary(Option.all()))\n",
    "data_path = Path(\"data\")\n",
    "metadata_path = data_path / \"metadata.csv\"\n",
    "if not metadata_path.is_file():\n",
    "   samples_df.to_csv(metadata_path, index=False, header=\"column_names\")\n",
    "else:\n",
    "   samples_df.to_csv(metadata_path, index=False, mode=\"a\", header=False)\n",
    "print(\"Number of resampled samples: \", len(samples_df))\n"
   ]
  },
  {
   "source": [
    "## Read dataset file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converters = {\n",
    "    column_name: ast.literal_eval\n",
    "    for column_name in [\"tag\", \"group\", \"parody\", \"character\", \"artist\", \"category\", \"image_urls\"]\n",
    "}\n",
    "hentais_df = pd.read_csv(metadata_path, converters=converters)\n",
    "hentais_df"
   ]
  },
  {
   "source": [
    "## Download images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, hentai in hentais_df.iterrows():\n",
    "    hentai_path = data_path / str(hentai.id)\n",
    "    if not hentai_path.is_dir():\n",
    "        hentai = Hentai(hentai.id)\n",
    "        hentai.download(hentai_path, progressbar=True)"
   ]
  },
  {
   "source": [
    "## Data preparation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_freq = hentais_df[\"tag\"].explode().value_counts().sort_values(ascending=False).head(50)\n",
    "\n",
    "style.use(\"fivethirtyeight\")\n",
    "plt.figure(figsize=(12, 20))\n",
    "sns.barplot(y=label_freq.index.values, x=label_freq, order=label_freq.index)\n",
    "plt.title(\"Label frequency\", fontsize=14)\n",
    "plt.xlabel(\"\")\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Data sparsity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nobjs = 2 # Maximum number of images to display\n",
    "ncols = 2 # Number of columns in display\n",
    "nrows = nobjs // ncols # Number of rows in display\n",
    "plt.figure(figsize=(14, 4 * nrows))\n",
    "hentais_df[\"num_favorites\"].plot.hist(ax=plt.subplot(nrows, ncols, 1), bins=100, title=\"Favorites\")\n",
    "hentais_df[\"num_pages\"].plot.hist(ax=plt.subplot(nrows, ncols, 2), bins=100, title=\"Pages\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "We need to complete the full path to locate training and test images from the current working directory."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hentais_df = hentais_df.explode(\"image_urls\").reset_index()\n",
    "filenames_df = hentais_df.apply(lambda x: str(Path(str(x[\"id\"])) / Path(x[\"image_urls\"]).name), axis=1).rename(\"filename\")\n",
    "labels_df = hentais_df[\"tag\"].rename(\"labels\")\n",
    "hentais_df = pd.concat([filenames_df, labels_df], axis=1)\n",
    "hentais_df"
   ]
  },
  {
   "source": [
    "## Image examples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nobjs = 8 # Maximum number of images to display\n",
    "ncols = 4 # Number of columns in display\n",
    "nrows = nobjs // ncols # Number of rows in display\n",
    "samples = hentais_df[\"filename\"].explode().apply(lambda x: str(data_path / x)).sample(nrows * ncols)\n",
    "plt.figure(figsize=(14, 4 * nrows))\n",
    "for i, img in enumerate(samples):\n",
    "    ax = plt.subplot(nrows, ncols, i+1)\n",
    "    ax.imshow(Image.open(img).convert(\"RGB\"))"
   ]
  },
  {
   "source": [
    "## Tensorflow DataSet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hentais_gen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 25 # Big enough to measure an F1-score\n",
    "IMG_SIZE = 224 # Specify height and width of image to match the input format of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = hentais_gen.flow_from_dataframe(\n",
    "    dataframe=hentais_df,\n",
    "    directory=\"data\",\n",
    "    x_col=\"filename\",\n",
    "    y_col=\"labels\",\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    shuffle=True,\n",
    "    seed=44,\n",
    "    subset=\"training\"\n",
    ")\n",
    "val_ds = hentais_gen.flow_from_dataframe(\n",
    "    dataframe=hentais_df,\n",
    "    directory=\"data\",\n",
    "    x_col=\"filename\",\n",
    "    y_col=\"labels\",\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    subset=\"validation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlabels = len(train_ds.class_indices)\n",
    "print(\"Number of hentais labels: \", nlabels)"
   ]
  },
  {
   "source": [
    "## Transfert learning feature extractor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    hub.KerasLayer(\n",
    "        \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "        trainable=False\n",
    "    ),\n",
    "    keras.layers.Dense(64, activation=\"relu\"),\n",
    "    keras.layers.Dense(nlabels, activation=\"sigmoid\")\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "## Train the model\n",
    "Specify the learning rate and the number of training epochs (number of loops over the whole dataset)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-5 # Keep it small when transfer learning\n",
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=LR), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"job\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "savedmodel_dir = output_dir / \"export/savedmodel\"\n",
    "model_export_path = savedmodel_dir / timestamp\n",
    "checkpoint_path = output_dir / \"checkpoints\"\n",
    "tensorboard_path = output_dir / \"tensorboard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(),\n",
    "    keras.callbacks.EarlyStopping(patience=2),\n",
    "    keras.callbacks.TensorBoard(str(tensorboard_path)),\n",
    "    keras.callbacks.ModelCheckpoint(str(checkpoint_path)),\n",
    "]\n",
    "\n",
    "history = model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.plot(history.history[\"accuracy\"])\n",
    "ax.plot(history.history[\"val_accuracy\"])\n",
    "ax.title(\"model accuracy\")\n",
    "ax.ylabel(\"accuracy\")\n",
    "ax.xlabel(\"epoch\")\n",
    "ax.legend([\"train\", \"validation\"])\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.plot(history.history[\"loss\"])\n",
    "ax.plot(history.history[\"val_loss\"])\n",
    "ax.title(\"model loss\")\n",
    "ax.ylabel(\"loss\")\n",
    "ax.xlabel(\"epoch\")\n",
    "ax.legend([\"train\", \"validation\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(model, str(model_export_path))"
   ]
  }
 ]
}