{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd07709ee700c0b35b705593aafecb5816646a1f77b192913c2314f04e0eb97f79e",
   "display_name": "Python 3.8.8 64-bit ('bepeka': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "192fa43bbebb6a2bf664879821bb9b226440b6277539cdbfb00fb0188b28dc5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils packages\n",
    "import ast\n",
    "from datetime import datetime\n",
    "from pathlib import Path, PurePath\n",
    "from typing import List\n",
    "\n",
    "# Visualization packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# Analysis packages\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "source": [
    "## Read dataset file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "DATA_PATH = \"data\" # Directory to drop collected data."
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hentais_df = pd.read_json(\n",
    "    Path(DATA_PATH) / \"preprocessed\" / \"metadata.ndjson\",\n",
    "    orient=\"records\",\n",
    "    lines=True\n",
    ")\n",
    "hentais_df"
   ]
  },
  {
   "source": [
    "## Data preparation\n",
    "\n",
    "The following preparation steps aim to create training and validation sets that can be used for machine learning.\n",
    "For example, if a hentai tag is rare, we will remove it from the target variable.\n",
    "The model will not learn how to predict that genre if the data covering it is insufficient."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_label_frequency(labels):\n",
    "    style.use(\"fivethirtyeight\")\n",
    "    plt.figure(figsize=(12, 16))\n",
    "    sns.barplot(y=labels.index.values, x=labels, order=labels.index)\n",
    "    plt.title(\"Labels\", fontsize=14)\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get label frequencies in descending order\n",
    "label_freq = hentais_df[\"tag\"].explode().value_counts()\n",
    "label_freq = label_freq / len(hentais_df[\"tag\"])\n",
    "plot_label_frequency(label_freq.head(70))"
   ]
  },
  {
   "source": [
    "## Remove infrequent labels\n",
    "\n",
    "We will consider as a rare label every label that is covered by less than 5% in our dataset.\n",
    "We will assume that rare labels are very hard to predict due to lack of sufficient data.\n",
    "The model that we will train later will not focus on predicting these labels.\n",
    "So, we need to make some transformation in the label column (tag) where we ignore infrequent labels by hiding them."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_label_df = label_freq[label_freq < 0.01]\n",
    "rare_label_df"
   ]
  },
  {
   "source": [
    "Remove the rare tags"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hentais_df = hentais_df.assign(\n",
    "    tag=hentais_df[\"tag\"].apply(lambda x: [l for l in x if l not in rare_label_df])\n",
    ")\n",
    "hentais_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 4))\n",
    "hentais_df[\"num_favorites\"].plot.hist(\n",
    "    ax=plt.subplot(1, 2, 1),\n",
    "    bins=100,\n",
    "    range=(0, 6000),\n",
    "    title=\"Favorites\"\n",
    ")\n",
    "hentais_df[\"num_pages\"].plot.hist(\n",
    "    ax=plt.subplot(1, 2, 2),\n",
    "    bins=100,\n",
    "    range=(0, 300),\n",
    "    title=\"Pages\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most frequent pages numbers: \", (hentais_df[\"num_pages\"]\n",
    "                                        .value_counts()\n",
    "                                        .idxmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hentais_df = (hentais_df\n",
    "              .explode(column=\"filenames\", ignore_index=True)\n",
    "              .rename(columns={\"filenames\": \"filename\"}))\n",
    "hentais_df"
   ]
  },
  {
   "source": [
    "## Image examples\n",
    "\n",
    "Let's display some examples of training images."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nobjs = 8 # Maximum number of images to display\n",
    "ncols = 4 # Number of columns in display\n",
    "nrows = nobjs // ncols # Number of rows in display\n",
    "\n",
    "plt.figure(figsize=(14, 4 * nrows))\n",
    "filenames_df = hentais_df[\"filename\"].sample(nrows * ncols)\n",
    "for i, filename in enumerate(filenames_df):\n",
    "    ax = plt.subplot(nrows, ncols, i + 1)\n",
    "    ax.imshow(Image.open(filename))"
   ]
  },
  {
   "source": [
    "## Train / val split\n",
    "\n",
    "Splitting the modeling data into training and validation is common in machine learning practice.\n",
    "We will be allocating 80% of the images for training and 20% for validation.\n",
    "Usually, we keep a final test set to communicate performance results but we will not really need it in this notebook."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = hentais_df[\"filename\"].to_numpy(), hentais_df[\"tag\"].to_numpy()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=44)\n",
    "print(\"Number of hentais for training: {}\".format(len(X_train)))\n",
    "print(\"Number of hentais for validation: {}\".format(len(X_val)))"
   ]
  },
  {
   "source": [
    "## Label encoding\n",
    "\n",
    "The original targets are lists of strings that can be easily understood by humans.\n",
    "But, if we want to build and train a neural network we need to create binary labels (multi-hot encoding).\n",
    "This is critical for multi-label classification.\n",
    "\n",
    "In order to binarize our labels, we will be using scikit-learn's MultiLabelBinarizer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the multi-label binarizer on the training set\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(Y)\n",
    "\n",
    "# Analyze the number of labels in the dataset for\n",
    "# the output layer of our subsequent model.\n",
    "labels_df = pd.Series(mlb.classes_)\n",
    "labels_df"
   ]
  },
  {
   "source": [
    "Analyze the number of labels in the dataset for the output layer of our subsequent model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlabels = len(labels_df)"
   ]
  },
  {
   "source": [
    "Transform the targets of the training and test sets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bin = mlb.transform(y_train)\n",
    "y_val_bin = mlb.transform(y_val)"
   ]
  },
  {
   "source": [
    "## Establish a baseline\n",
    "\n",
    "Following development best practices, you should establish a baseline. The simplest baseline is predicting the most average classes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bin = mlb.transform(hentais_df[\"tag\"])\n",
    "print(\"Average class prediction baseline: {0:.0%}\".format(np.mean(y_bin)))"
   ]
  },
  {
   "source": [
    "## Input pipeline\n",
    "\n",
    "If you are familiar with keras.preprocessing you may know the image data iterators (E.g. ImageDataGenerator, DirectoryIterator).\n",
    "These iterators are convenient for multi-class classfication where the image directory contains one subdirectory for each class.\n",
    "But, in the case of multi-label classification, having an image directory that respects this structure is not possible because one observation can belong to multiple classes at the same time.\n",
    "\n",
    "That is where the tf.data API has the upper hand.\n",
    "- It is faster.\n",
    "- It provides fine-grained control.\n",
    "- It is well integrated with the rest of TensorFlow.\n",
    "\n",
    "We first need to write some function to parse image files and generate a tensor representing the features and a tensor representing the labels.\n",
    "- In this function we can resize the image to adapt to the input expected by the model.\n",
    "- We can also normalize the pixel values to be between 0 and 1. This is a common practice that helps speed up the convergence of training.\n",
    "\n",
    "If we consider every pixel as a feature, we would like these features to have a similar range so that the gradients don't go out of control and that we only need one global learning rate multiplier."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_TIMESTEP = 1\n",
    "IMG_SIZE = 224 # Specify height and width of image to match the input format of the model\n",
    "CHANNELS = 3 # Keep RGB color channels to match the input format of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function(filename: tf.Tensor, label: tf.Tensor):\n",
    "    # Read an image from a file\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    # Decode it into a dense vector\n",
    "    image_decoded = tf.image.decode_image(\n",
    "        image_string,\n",
    "        channels=CHANNELS,\n",
    "        expand_animations=False\n",
    "    )\n",
    "    # Resize it to fixed shape\n",
    "    image_resized = tf.image.resize(image_decoded, [IMG_SIZE, IMG_SIZE])\n",
    "    # Normalize it from [0, 255] to [0.0, 1.0]\n",
    "    image_normalized = image_resized / 255.0\n",
    "    return image_normalized, label"
   ]
  },
  {
   "source": [
    "To train a model on our dataset we want the data to be:\n",
    "\n",
    "- Well shuffled\n",
    "- Batched\n",
    "- Batches to be available as soon as possible.\n",
    "\n",
    "These features can be easily added using the tf.data.Dataset abstraction."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256 # Big enough to measure an F1-score\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE # Adapt preprocessing and prefetching dynamically\n",
    "SHUFFLE_BUFFER_SIZE = 1024 # Shuffle the training data by a chunck of 1024 observations"
   ]
  },
  {
   "source": [
    "AUTOTUNE will adapt the preprocessing and prefetching workload to model training and batch consumption.\n",
    "The number of elements to prefetch should be equal to (or possibly greater than) the number of batches consumed by a single training step.\n",
    "AUTOTUNE will prompt the tf.data runtime to tune the value dynamically at runtime and reduce GPU and CPU idle time.\n",
    "\n",
    "We can now create a function that generates training and validation datasets for TensorFlow."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(filenames: tf.Tensor, labels: tf.Tensor, is_training=True):\n",
    "    # Create a second dataset of labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    # Parse and preprocess observations in parallel\n",
    "    dataset = dataset.map(parse_function, num_parallel_calls=AUTOTUNE)\n",
    "    if is_training == True:\n",
    "        # Shuffle the data each buffer size\n",
    "        dataset = dataset.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
    "    # Batch the data for multiple steps\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # Fetch batches in the background while the model is training.\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = create_dataset(X_train, y_train_bin)\n",
    "val_ds = create_dataset(X_val, y_val_bin)"
   ]
  },
  {
   "source": [
    "## Transfert Learning\n",
    "\n",
    "The feature extractor accepts images of shape (224, 224, 3) and returns a 1280-length vector for each image.\n",
    "\n",
    "We should freeze the variables in the feature extractor layer, so that the training only modifies the new classification layers.\n",
    "Usually, it is a good practice when working with datasets that are very small compared to the orginal dataset the feature extractor was trained on."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_layer = keras.applications.InceptionV3(\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, CHANNELS),\n",
    "    include_top=False,\n",
    "    pooling=\"max\"\n",
    ")\n",
    "feature_extractor_layer.trainable = False"
   ]
  },
  {
   "source": [
    "## Main Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    # layers.TimeDistributed(\n",
    "    #     feature_extractor_layer,\n",
    "    #     input_shape=(IMG_TIMESTEP, IMG_SIZE, IMG_SIZE, CHANNELS)\n",
    "    # ),\n",
    "    # layers.GRU(64),\n",
    "    # layers.Dense(1024, activation=\"relu\"),\n",
    "    # layers.Dropout(0.5),\n",
    "    # layers.Dense(512, activation=\"relu\"),\n",
    "    # layers.Dropout(0.5),\n",
    "    # layers.Dense(128, activation=\"relu\"),\n",
    "    # layers.Dropout(0.5),\n",
    "    # layers.Dense(64, activation=\"relu\"),\n",
    "    # layers.Dense(nlabels, activation=\"sigmoid\")\n",
    "    feature_extractor_layer,\n",
    "    layers.Dense(1024, activation=\"relu\"),\n",
    "    layers.Dense(nlabels, activation=\"sigmoid\")\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "## Model training and evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def macro_soft_f1(y: tf.Tensor, y_hat: tf.Tensor):\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.cast(y_hat, tf.float32)\n",
    "    tp = tf.reduce_sum(y_hat * y, axis=0)\n",
    "    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n",
    "    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n",
    "    soft_f1 = 2*tp / (2 * tp + fn + fp + 1e-16)\n",
    "    # Reduce 1 - soft-f1 in order to increase soft-f1\n",
    "    cost = 1 - soft_f1\n",
    "    # Average on all labels\n",
    "    macro_cost = tf.reduce_mean(cost)\n",
    "    return macro_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def macro_f1(y: tf.Tensor, y_hat: tf.Tensor, thresh=0.5):\n",
    "    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
    "    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)\n",
    "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)\n",
    "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)\n",
    "    f1 = 2*tp / (2 * tp + fn + fp + 1e-16)\n",
    "    macro_f1 = tf.reduce_mean(f1)\n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "EPOCHS = 300"
   ]
  },
  {
   "source": [
    "Compile the model to configure the training process."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "  loss=macro_soft_f1,\n",
    "  metrics=[macro_f1]\n",
    ")"
   ]
  },
  {
   "source": [
    "Now, we pass the training dataset of (features, labels) to fit the model and indicate a seperate dataset for validation.\n",
    "The performance on the validation dataset will be measured after each epoch."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"job\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "savedmodel_dir = output_dir / \"export\" / \"savedmodel\"\n",
    "model_export_path = savedmodel_dir / timestamp\n",
    "checkpoint_path = output_dir / \"checkpoints\"\n",
    "tensorboard_path = output_dir / \"tensorboard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[\n",
    "        keras.callbacks.ReduceLROnPlateau(),\n",
    "        keras.callbacks.EarlyStopping(patience=10),\n",
    "        keras.callbacks.TensorBoard(\n",
    "            str(tensorboard_path),\n",
    "            histogram_freq=1\n",
    "        ),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            str(checkpoint_path),\n",
    "            save_weights_only=True,\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.plot(history.history[\"macro_f1\"])\n",
    "ax.plot(history.history[\"val_macro_f1\"])\n",
    "ax.set_title(\"Macro F1\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.legend([\"train\", \"validation\"])\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.plot(history.history[\"loss\"])\n",
    "ax.plot(history.history[\"val_loss\"])\n",
    "ax.set_title(\"Loss\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.legend([\"train\", \"validation\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(str(model_export_path))"
   ]
  },
  {
   "source": [
    "## Show predictions\n",
    "\n",
    "We can try and see what the predictions will look like when using our model on pages of some known hentais.\n",
    "The following function simplifies the process of preparing images data, generating the prediction from the model and visualizing it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(str(model_export_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(path: str):\n",
    "    img = keras.preprocessing.image.load_img(\n",
    "        path,\n",
    "        target_size=(IMG_SIZE,IMG_SIZE,CHANNELS)\n",
    "    )\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "    img = img / 255\n",
    "    return img\n",
    "\n",
    "def plot_prediction(title: str):\n",
    "    # Get hentai info\n",
    "    hentai = hentais_df[hentais_df[\"title\"]==title]\n",
    "    id = hentai[\"id\"].iloc[0]\n",
    "    genre = hentai[\"tag\"].iloc[0]\n",
    "    img_paths = np.arange(1, 11)\n",
    "    img_paths = np.array([\n",
    "        Path(\"data\") / \"preprocessed\" / str(id) / \"{}.jpg\".format(x)\n",
    "        for x in img_paths\n",
    "    ])\n",
    "\n",
    "    # Read and prepare image\n",
    "    imgs = decode_image(img_paths[0])\n",
    "    imgs = np.expand_dims(imgs, axis=0)\n",
    "\n",
    "    # Generate prediction\n",
    "    prediction = model.predict(imgs)\n",
    "    prediction = pd.Series(prediction[0])\n",
    "    prediction = pd.concat(\n",
    "        [labels_df.rename(\"label\"), prediction.rename(\"prediction\")],\n",
    "        axis=1\n",
    "    )\n",
    "    prediction = prediction[prediction[\"prediction\"] > 0.5]\n",
    "    prediction = prediction[\"label\"]\n",
    "\n",
    "    # Dispaly image with prediction\n",
    "    style.use(\"default\")\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.imshow(Image.open(img_paths[0]))\n",
    "    plt.title(\n",
    "        \"{}\\n\\nGenre\\n{}\\n\\nPrediction\\n{}\\n\".format(title, genre, list(prediction)),\n",
    "         fontsize=9\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\n",
    "    \"My Little Brother\",\n",
    "    \"Maotoko Rental Service\",\n",
    "    \"Maeoki wa Iranu Warawa to Asobe\",\n",
    "    \"Jigoku e no Katamichi 1 Credit\",\n",
    "    \"Makuu GB Tsuushin 3\"\n",
    "]\n",
    "\n",
    "for title in titles:\n",
    "    plot_prediction(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}